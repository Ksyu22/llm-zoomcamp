# Module Outline: Vector Search

### Theory

1. **Embeddings**:
    - Vector representation of text (other types of data). It's a way to translate textual representation into numerical one and allows to use this data with ML/DL algorithms.
    - Can be created with different embeddings model(word, sentence, document,...)

2. **Vector DataBase**:
    - Indexes and stores embeddings in a very optimal way which permits a very effective search
    - Allows to perform similarity search btwn elements
    - Can support LLMs with additional and easily available knowledge that LLMs might lack

  ***Indexing Embeddings***:
      Index our documents or some fields with embeddings and after perform query with vector representation for search.

  ***Vector DataBase Search***:
      Vector embedding search permits to improve search relevance because it captures semantic meaning, which is not available with usual keyword search.


3. **Offline Evaluation of Retrieval**:
    - Methods for evaluating the effectiveness of retrieval systems.
    - Metrics for assessing retrieval performance (e.g., precision, recall, F1-score).
    - Setting up offline evaluation experiments to benchmark retrieval systems.

### Practice

1. [**Vector Search with Elasticsearch**](https://github.com/Ksyu22/llm-zoomcamp/blob/main/03-vector-search/elastic_search.ipynb):
   - Introduction to Elasticsearch and its capabilities for handling vector search.
   - Configuring Elasticsearch for embedding-based search.
   - Querying and retrieving documents using vector search in Elasticsearch.

2. **Offline Evaluation of Retrieval**:

  1. Generation of the **ground truth** dataset on the basis of the actual records from our knowledge database.

  **Strategy:** For every record(user question in FAQ) we are prompting gpt-4o-mini to generate similar/relevant records(questions).

  **Challenges:**
  - During this data preprocessing we had to deal with different possible unique indexes generation approaches (record indexes were not initially retrieved) in order to be able to build a lookup table for our evaluation task.
  - Some records didn't get any relevant questions generated by LLM. Instead we got `question` value. These records were deleted from the ground truth dataset.
